{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "replicate_double_descent_fully_connected_less_epochs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "komPxXzy6oTT",
        "outputId": "204ba956-67d1-467c-a3da-5910c846dbd9"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow.keras.layers import Dense, Flatten \n",
        "from tensorflow.keras import Model\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYkDDKhP6ozO"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vqNW9NH6rvs"
      },
      "source": [
        "# paper's fully connected model learned on a subset of MNIST (n = 4000, d = 784, K = 10 classes)\n",
        "x_train, y_train = x_train[:4000], y_train[:4000]\n",
        "x_test, y_test = x_test[:4000], y_test[:4000]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y6-hXm1t6vf_",
        "outputId": "30cc8e08-11a8-47f7-ae75-f477cf48c9c0"
      },
      "source": [
        "# For networks smaller than the interpolation threshold, we decay the step size by 10% after each of 500 epochs\n",
        "# For these networks, training is stopped after classification error reached zero or 6000 epochs, whichever happens earlier. For networks larger than interpolation threshold, fixed step size is used, and training is stopped after 6000 epochs.\n",
        "def lr_decay(epoch, lr):\n",
        "    return lr * 0.9**(epoch//500)\n",
        "scheduler = tf.keras.callbacks.LearningRateScheduler(lr_decay)\n",
        "\n",
        "# For networks smaller than the interpolation threshold, training is stopped after classification error reached zero or 6000 epochs,\n",
        "class CustomCallback_epoch(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      if logs[\"accuracy\"]==1:\n",
        "        self.model.stop_training = True\n",
        "        print('STOPPING EARLY AFTER INTERPOLATING AT EPOCH %d' %epoch)\n",
        "        print('INTERPOLATION THRESHOLD REACHED')\n",
        "\n",
        "history_logs = []\n",
        "saved_weights = []\n",
        "\n",
        "num_parameters = [i*1000 for i in [3, 6, 9, 12, 24, 28, 32, 34, 36, 38, 40, 42, 46, 50, 80, 150, 300, 800]]\n",
        "hidden_sizes = [(N-10)//795 for N in num_parameters]\n",
        "for N in num_parameters:\n",
        "  print('='*80)\n",
        "  print('Number of parameters = %d, %d hidden layer neurons' %(N, (N-10)//795)) \n",
        "  print('='*80)\n",
        "\n",
        "  # P = (d+1)·H+(H+1)·K = 785H+10H+10 = 795H+10 --> H = (P-10)/795\n",
        "  num_nodes = (N-10)//795\n",
        "  # The remaining weights are initialized with normally distributed random numbers (mean 0 and variance 0.01). The smallest network is initialized using standard Glorotuniform distribution [19].\n",
        "  normal_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01**0.5)\n",
        "  glorot_initializer = tf.keras.initializers.GlorotUniform()  \n",
        "  # if this is the first model (smallest network) use standard Glorotuniform, otherwise use random normal\n",
        "  initializer = normal_initializer if saved_weights else glorot_initializer\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(num_nodes, activation='relu', kernel_initializer=initializer, bias_initializer=initializer),\n",
        "    tf.keras.layers.Dense(10, activation='softmax', kernel_initializer=initializer, bias_initializer=initializer)\n",
        "  ])\n",
        "\n",
        "  # paper used SGD with standard momentum (parameter value 0.95)\n",
        "  opt = tf.keras.optimizers.SGD(momentum=0.95)\n",
        "\n",
        "  model.compile(optimizer=opt,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  # print(model.weights[0].shape)\n",
        "  # print(model.weights[1].shape)\n",
        "  # # print(model.weights[1])\n",
        "  # print(model.weights[2].shape)\n",
        "  # # print(model.weights[2])\n",
        "  # print(model.weights[3].shape)\n",
        "  # # print(model.weights[3])\n",
        "\n",
        "  # from paper: To train a larger network with H2 > H1 hidden units, we initialize the first H1 hidden units of the larger network to the weights learned in the smaller network.  \n",
        "  # equivalently, here we expand saved weight with H1 units from prev model with randomly initialized weight for the new hidden units from the new model, then re-assign the combined weights to the new model\n",
        "  if saved_weights: \n",
        "    prev_weights = saved_weights[-1]\n",
        "    tf.compat.v1.assign(model.weights[0], tf.concat((prev_weights[0], model.weights[0][:, prev_weights[0].shape[1]:]), axis=1))\n",
        "    tf.compat.v1.assign(model.weights[1], tf.concat((prev_weights[1], model.weights[1][prev_weights[1].shape[0]:]), axis=0))\n",
        "    tf.compat.v1.assign(model.weights[2], tf.concat((prev_weights[2], model.weights[2][prev_weights[2].shape[0]:]), axis=0))\n",
        "    tf.compat.v1.assign(model.weights[3], prev_weights[3])\n",
        "\n",
        "# For networks smaller than the interpolation threshold, we decay the step size by 10% after each of 500 epochs,\n",
        "# For these networks, training is stopped after classification error reached zero or 6000 epochs, whichever happens earlier. For networks larger than interpolation threshold, fixed step size is used, and training is stopped after 6000 epochs.\n",
        "\n",
        "  interpolated = history_logs and max([h[1].history['accuracy'] for h in history_logs])[0] == 1.0\n",
        "  # interpolated = N >= 40000\n",
        "  if not interpolated:\n",
        "    history = model.fit(x_train, y_train, epochs=1000, validation_data=(x_test, y_test), verbose=0, callbacks=[CustomCallback_epoch(), scheduler])\n",
        "  else:\n",
        "    history = model.fit(x_train, y_train, epochs=1000, validation_data=(x_test, y_test), verbose=0)\n",
        "  \n",
        "  saved_weights.append(model.weights)\n",
        "\n",
        "  history_logs.append((N, history))\n",
        "  print(tf.math.confusion_matrix(y_test, tf.argmax(model.predict(x_test), axis=1)))\n",
        "  model.summary()\n",
        "  print('max training accuracy', max(history.history['accuracy']))\n",
        "  print('min training loss', min(history.history['loss']))\n",
        "  print('max validation accuracy', max(history.history['val_accuracy']))\n",
        "  print('min validation loss', min(history.history['val_loss']))\n",
        "  print()\n",
        "  print('last training accuracy', history.history['accuracy'][-1])\n",
        "  print('last training loss', history.history['loss'][-1])\n",
        "  print('last validation accuracy', history.history['val_accuracy'][-1])\n",
        "  print('last validation loss', history.history['val_loss'][-1])\n",
        "  \n",
        "# plot errors over number of parameters\n",
        "plt.title('Training and validation error')\n",
        "plt.plot([i[0]//1000 for i in history_logs], [1-i[1].history['val_accuracy'][-1] for i in history_logs], color='blue', label='val_error')\n",
        "plt.plot([i[0]//1000 for i in history_logs], [1-i[1].history['accuracy'][-1] for i in history_logs], color='orange', label='training error')\n",
        "plt.xlabel('Number of parameters N(*10^3)')\n",
        "plt.ylabel('Error')\n",
        "plt.legend()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Number of parameters = 3000, 3 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[275   0   0  13   3  64   2   1   4   8]\n",
            " [  0 402   8   3   8   1   8   4  10   6]\n",
            " [  1  28 275  49   2   5  10  11  34   3]\n",
            " [ 26   8  32 233   1  21   0  29  45  13]\n",
            " [  0  12   0   0 292   3  25   2   5  79]\n",
            " [ 52   0   3  20  16 191  15   6  51  18]\n",
            " [  4  13   9   1  41  43 235   2  23   7]\n",
            " [  2  25   6  25   8   1   0 303   2  39]\n",
            " [  8  28  34  35  12  47  16   6 191   7]\n",
            " [  7  11   0  14  51   9   3  37   4 255]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_99\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_99 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_198 (Dense)            (None, 3)                 2355      \n",
            "_________________________________________________________________\n",
            "dense_199 (Dense)            (None, 10)                40        \n",
            "=================================================================\n",
            "Total params: 2,395\n",
            "Trainable params: 2,395\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 0.9267500042915344\n",
            "min training loss 0.2536751925945282\n",
            "max validation accuracy 0.7114999890327454\n",
            "min validation loss 0.9404743909835815\n",
            "\n",
            "last training accuracy 0.925000011920929\n",
            "last training loss 0.25367528200149536\n",
            "last validation accuracy 0.6629999876022339\n",
            "last validation loss 1.942470669746399\n",
            "================================================================================\n",
            "Number of parameters = 6000, 7 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[334   0  13   1   5   5   6   1   0   5]\n",
            " [  0 432   2   4   2   1   4   0   5   0]\n",
            " [  8  16 338  17   4   3   5  10  14   3]\n",
            " [  3   6  20 294   2  47   0  12  10  14]\n",
            " [  2   8   6   0 352   1   8   6   5  30]\n",
            " [ 17   1   2  28   9 259  15   5  17  19]\n",
            " [ 16   7  21   0  12  20 293   2   4   3]\n",
            " [  6  14  19  11  11   1   3 317   2  27]\n",
            " [  5   9  24  17  13  24   6   7 265  14]\n",
            " [  5   2   1  11  24   4   6  19  13 306]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_100\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_100 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_200 (Dense)            (None, 7)                 5495      \n",
            "_________________________________________________________________\n",
            "dense_201 (Dense)            (None, 10)                80        \n",
            "=================================================================\n",
            "Total params: 5,575\n",
            "Trainable params: 5,575\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 0.999750018119812\n",
            "min training loss 0.0012016503605991602\n",
            "max validation accuracy 0.7994999885559082\n",
            "min validation loss 1.4252115488052368\n",
            "\n",
            "last training accuracy 0.999750018119812\n",
            "last training loss 0.0012016510590910912\n",
            "last validation accuracy 0.7975000143051147\n",
            "last validation loss 2.5071096420288086\n",
            "================================================================================\n",
            "Number of parameters = 9000, 11 hidden layer neurons\n",
            "================================================================================\n",
            "STOPPING EARLY AFTER INTERPOLATING AT EPOCH 22\n",
            "INTERPOLATION THRESHOLD REACHED\n",
            "tf.Tensor(\n",
            "[[334   0  13   1   5   5   6   1   0   5]\n",
            " [  0 432   2   4   2   1   4   0   5   0]\n",
            " [  7  16 338  17   4   4   5  10  14   3]\n",
            " [  3   6  20 294   3  47   1  12  10  12]\n",
            " [  2   8   6   0 352   1   8   5   5  31]\n",
            " [ 16   1   2  28   9 259  15   5  18  19]\n",
            " [ 15   7  22   0  11  20 294   2   4   3]\n",
            " [  6  14  19   9  11   1   3 317   2  29]\n",
            " [  5   9  24  17  13  24   7   7 265  13]\n",
            " [  5   3   1  11  24   4   6  18  13 306]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_101\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_101 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_202 (Dense)            (None, 11)                8635      \n",
            "_________________________________________________________________\n",
            "dense_203 (Dense)            (None, 10)                120       \n",
            "=================================================================\n",
            "Total params: 8,755\n",
            "Trainable params: 8,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 0.0010427661472931504\n",
            "max validation accuracy 0.7987499833106995\n",
            "min validation loss 2.506523609161377\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 0.0010427661472931504\n",
            "last validation accuracy 0.7977499961853027\n",
            "last validation loss 2.5215976238250732\n",
            "================================================================================\n",
            "Number of parameters = 12000, 15 hidden layer neurons\n",
            "================================================================================\n",
            "STOPPING EARLY AFTER INTERPOLATING AT EPOCH 1\n",
            "INTERPOLATION THRESHOLD REACHED\n",
            "tf.Tensor(\n",
            "[[334   0  13   1   5   5   6   1   0   5]\n",
            " [  0 432   2   4   2   1   4   0   5   0]\n",
            " [  7  16 338  16   3   5   5  11  14   3]\n",
            " [  3   6  20 294   3  47   0  12  10  13]\n",
            " [  2   8   6   0 351   2   8   5   5  31]\n",
            " [ 16   1   2  28   9 259  15   5  18  19]\n",
            " [ 14   7  21   0  12  20 295   2   4   3]\n",
            " [  6  14  19   9  10   1   3 318   2  29]\n",
            " [  5   9  24  17  13  23   7   6 265  15]\n",
            " [  5   4   1  11  23   4   6  18  10 309]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_102\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_102 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_204 (Dense)            (None, 15)                11775     \n",
            "_________________________________________________________________\n",
            "dense_205 (Dense)            (None, 10)                160       \n",
            "=================================================================\n",
            "Total params: 11,935\n",
            "Trainable params: 11,935\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 0.0010436538141220808\n",
            "max validation accuracy 0.7987499833106995\n",
            "min validation loss 2.524062156677246\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 0.0010436538141220808\n",
            "last validation accuracy 0.7987499833106995\n",
            "last validation loss 2.525790214538574\n",
            "================================================================================\n",
            "Number of parameters = 24000, 30 hidden layer neurons\n",
            "================================================================================\n",
            "STOPPING EARLY AFTER INTERPOLATING AT EPOCH 0\n",
            "INTERPOLATION THRESHOLD REACHED\n",
            "tf.Tensor(\n",
            "[[335   0  12   1   5   5   6   1   0   5]\n",
            " [  0 432   2   4   2   1   4   0   5   0]\n",
            " [  6  16 340  15   4   4   5  11  14   3]\n",
            " [  3   6  21 293   3  48   0  12   9  13]\n",
            " [  2   8   6   0 353   1   9   5   4  30]\n",
            " [ 16   1   2  28   9 260  15   5  17  19]\n",
            " [ 14   7  21   0  13  20 294   2   4   3]\n",
            " [  6  14  19   9  10   1   3 317   2  30]\n",
            " [  5   9  24  17  16  23   6   6 264  14]\n",
            " [  5   4   1  11  23   4   6  18  10 309]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_103\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_103 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_206 (Dense)            (None, 30)                23550     \n",
            "_________________________________________________________________\n",
            "dense_207 (Dense)            (None, 10)                310       \n",
            "=================================================================\n",
            "Total params: 23,860\n",
            "Trainable params: 23,860\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 0.0010640800464898348\n",
            "max validation accuracy 0.7992500066757202\n",
            "min validation loss 2.5272414684295654\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 0.0010640800464898348\n",
            "last validation accuracy 0.7992500066757202\n",
            "last validation loss 2.5272414684295654\n",
            "================================================================================\n",
            "Number of parameters = 28000, 35 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[335   0  12   1   5   5   6   1   0   5]\n",
            " [  0 430   2   5   2   1   4   0   6   0]\n",
            " [  7  15 343  14   4   4   5  10  13   3]\n",
            " [  4   3  19 301   4  43   1  13   9  11]\n",
            " [  1   3   7   1 351   1  10   6   6  32]\n",
            " [ 16   1   3  26  11 259  13   6  18  19]\n",
            " [ 15   6  21   0  11  18 299   2   4   2]\n",
            " [  6  14  21   8  10   1   2 318   2  29]\n",
            " [  5  10  23  18  13  24   6   5 267  13]\n",
            " [  5   2   1  12  24   3   7  17  11 309]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_104\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_104 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_208 (Dense)            (None, 35)                27475     \n",
            "_________________________________________________________________\n",
            "dense_209 (Dense)            (None, 10)                360       \n",
            "=================================================================\n",
            "Total params: 27,835\n",
            "Trainable params: 27,835\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 9.138023597188294e-05\n",
            "max validation accuracy 0.8032500147819519\n",
            "min validation loss 2.5274174213409424\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 9.138023597188294e-05\n",
            "last validation accuracy 0.8029999732971191\n",
            "last validation loss 2.816549777984619\n",
            "================================================================================\n",
            "Number of parameters = 32000, 40 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[335   0  12   1   5   5   6   1   0   5]\n",
            " [  0 431   3   5   1   1   4   0   5   0]\n",
            " [  7  15 343  14   4   4   5  10  13   3]\n",
            " [  4   3  17 303   4  42   1  14  10  10]\n",
            " [  1   3   7   1 352   1  10   5   6  32]\n",
            " [ 17   1   3  26  10 262  12   6  18  17]\n",
            " [ 15   6  21   0   9  18 301   2   4   2]\n",
            " [  6  14  21   7  12   1   2 319   1  28]\n",
            " [  5  10  22  20  13  23   6   3 269  13]\n",
            " [  5   2   1  12  24   3   7  15  11 311]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_105\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_105 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_210 (Dense)            (None, 40)                31400     \n",
            "_________________________________________________________________\n",
            "dense_211 (Dense)            (None, 10)                410       \n",
            "=================================================================\n",
            "Total params: 31,810\n",
            "Trainable params: 31,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 4.540925147011876e-05\n",
            "max validation accuracy 0.8067499995231628\n",
            "min validation loss 2.8189845085144043\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 4.540925147011876e-05\n",
            "last validation accuracy 0.8065000176429749\n",
            "last validation loss 2.9198005199432373\n",
            "================================================================================\n",
            "Number of parameters = 34000, 42 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[336   0  11   1   5   5   6   1   0   5]\n",
            " [  0 431   3   5   1   1   4   0   5   0]\n",
            " [  7  15 343  14   4   4   5  10  13   3]\n",
            " [  4   3  17 304   4  43   1  13  10   9]\n",
            " [  1   3   7   1 353   1  10   5   7  30]\n",
            " [ 17   1   3  25  10 263  12   6  18  17]\n",
            " [ 15   6  21   0  10  18 300   2   4   2]\n",
            " [  6  14  20   7  12   1   2 320   1  28]\n",
            " [  5  10  22  19  13  23   6   3 270  13]\n",
            " [  5   2   1  12  23   3   7  16  11 311]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_106\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_106 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_212 (Dense)            (None, 42)                32970     \n",
            "_________________________________________________________________\n",
            "dense_213 (Dense)            (None, 10)                430       \n",
            "=================================================================\n",
            "Total params: 33,400\n",
            "Trainable params: 33,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 2.938364923465997e-05\n",
            "max validation accuracy 0.8077499866485596\n",
            "min validation loss 2.9189438819885254\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 2.938364923465997e-05\n",
            "last validation accuracy 0.8077499866485596\n",
            "last validation loss 2.9829249382019043\n",
            "================================================================================\n",
            "Number of parameters = 36000, 45 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[337   0  11   1   5   5   6   1   0   4]\n",
            " [  0 431   3   5   1   1   4   0   5   0]\n",
            " [  7  15 344  13   4   4   5  10  13   3]\n",
            " [  4   3  17 305   4  43   1  13  10   8]\n",
            " [  1   3   7   1 353   1  10   5   7  30]\n",
            " [ 17   1   3  24   9 264  12   6  18  18]\n",
            " [ 15   6  21   0  10  18 300   2   4   2]\n",
            " [  6  13  20   7  12   1   2 321   1  28]\n",
            " [  5   9  22  18  13  22   5   3 273  14]\n",
            " [  5   2   1  12  23   3   6  17  11 311]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_107\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_107 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_214 (Dense)            (None, 45)                35325     \n",
            "_________________________________________________________________\n",
            "dense_215 (Dense)            (None, 10)                460       \n",
            "=================================================================\n",
            "Total params: 35,785\n",
            "Trainable params: 35,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 2.1524016119656153e-05\n",
            "max validation accuracy 0.8097500205039978\n",
            "min validation loss 2.982827663421631\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 2.1524016119656153e-05\n",
            "last validation accuracy 0.8097500205039978\n",
            "last validation loss 3.0275216102600098\n",
            "================================================================================\n",
            "Number of parameters = 38000, 47 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[337   0  12   0   5   5   6   1   0   4]\n",
            " [  0 431   3   5   1   1   4   0   5   0]\n",
            " [  7  15 344  13   4   4   5  10  13   3]\n",
            " [  4   3  17 305   4  43   1  13  10   8]\n",
            " [  1   3   7   1 354   1  10   4   7  30]\n",
            " [ 17   1   3  23   9 265  12   6  18  18]\n",
            " [ 15   6  21   0  10  18 300   2   4   2]\n",
            " [  6  13  20   7  12   1   2 322   1  27]\n",
            " [  5  10  21  18  13  22   5   3 273  14]\n",
            " [  5   2   1  12  23   3   5  17  11 312]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_108\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_108 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_216 (Dense)            (None, 47)                36895     \n",
            "_________________________________________________________________\n",
            "dense_217 (Dense)            (None, 10)                480       \n",
            "=================================================================\n",
            "Total params: 37,375\n",
            "Trainable params: 37,375\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 1.685309143795166e-05\n",
            "max validation accuracy 0.8107500076293945\n",
            "min validation loss 3.0291197299957275\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 1.685309143795166e-05\n",
            "last validation accuracy 0.8107500076293945\n",
            "last validation loss 3.0622973442077637\n",
            "================================================================================\n",
            "Number of parameters = 40000, 50 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[337   0  12   0   5   5   6   1   0   4]\n",
            " [  0 431   4   4   1   1   4   0   5   0]\n",
            " [  7  15 343  14   4   4   5  10  13   3]\n",
            " [  5   3  17 304   4  43   1  13  10   8]\n",
            " [  1   4   7   1 354   1  10   4   6  30]\n",
            " [ 17   0   3  23   9 265  11   6  19  19]\n",
            " [ 15   6  22   0  10  17 301   1   4   2]\n",
            " [  6  14  20   7  11   1   2 322   1  27]\n",
            " [  5   8  22  19  13  22   5   3 273  14]\n",
            " [  5   2   1  12  24   3   4  17  11 312]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_109\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_109 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_218 (Dense)            (None, 50)                39250     \n",
            "_________________________________________________________________\n",
            "dense_219 (Dense)            (None, 10)                510       \n",
            "=================================================================\n",
            "Total params: 39,760\n",
            "Trainable params: 39,760\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 1.387083284498658e-05\n",
            "max validation accuracy 0.8105000257492065\n",
            "min validation loss 3.0649733543395996\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 1.387083284498658e-05\n",
            "last validation accuracy 0.8105000257492065\n",
            "last validation loss 3.0914082527160645\n",
            "================================================================================\n",
            "Number of parameters = 42000, 52 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[337   0  12   0   5   5   6   1   0   4]\n",
            " [  0 431   4   4   1   1   4   0   5   0]\n",
            " [  7  15 343  14   4   4   5  10  13   3]\n",
            " [  5   3  17 304   4  43   1  14  10   7]\n",
            " [  1   4   6   1 355   1  10   4   6  30]\n",
            " [ 17   0   3  23   9 265  11   6  19  19]\n",
            " [ 15   6  22   0  10  17 301   1   4   2]\n",
            " [  6  14  20   7  11   1   2 322   1  27]\n",
            " [  5   8  21  18  13  22   5   3 275  14]\n",
            " [  5   2   1  12  23   3   4  17  11 313]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_110\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_110 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_220 (Dense)            (None, 52)                40820     \n",
            "_________________________________________________________________\n",
            "dense_221 (Dense)            (None, 10)                530       \n",
            "=================================================================\n",
            "Total params: 41,350\n",
            "Trainable params: 41,350\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 1.1697989066306036e-05\n",
            "max validation accuracy 0.8115000128746033\n",
            "min validation loss 3.091679573059082\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 1.1698736670950893e-05\n",
            "last validation accuracy 0.8115000128746033\n",
            "last validation loss 3.1148812770843506\n",
            "================================================================================\n",
            "Number of parameters = 46000, 57 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[337   0  12   0   5   5   6   1   0   4]\n",
            " [  0 431   4   4   1   1   4   0   5   0]\n",
            " [  7  15 345  13   4   4   5   9  13   3]\n",
            " [  6   3  17 304   4  43   1  12  10   8]\n",
            " [  1   3   6   1 355   1  10   4   7  30]\n",
            " [ 17   0   3  22   9 266  11   6  19  19]\n",
            " [ 14   5  22   0  10  18 301   2   4   2]\n",
            " [  6  14  20   7  11   1   2 321   1  28]\n",
            " [  5   8  21  18  13  22   5   4 275  13]\n",
            " [  5   2   1  12  23   3   4  16  11 314]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_111\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_111 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_222 (Dense)            (None, 57)                44745     \n",
            "_________________________________________________________________\n",
            "dense_223 (Dense)            (None, 10)                580       \n",
            "=================================================================\n",
            "Total params: 45,325\n",
            "Trainable params: 45,325\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 1.0108020433108322e-05\n",
            "max validation accuracy 0.812250018119812\n",
            "min validation loss 3.1114232540130615\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 1.0108020433108322e-05\n",
            "last validation accuracy 0.812250018119812\n",
            "last validation loss 3.1370959281921387\n",
            "================================================================================\n",
            "Number of parameters = 50000, 62 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[337   0  12   0   5   5   6   1   0   4]\n",
            " [  0 432   4   4   1   1   4   0   4   0]\n",
            " [  7  15 344  14   4   4   5   9  13   3]\n",
            " [  6   3  17 305   4  43   1  11  10   8]\n",
            " [  1   2   6   1 356   1  10   4   7  30]\n",
            " [ 17   0   2  21  10 266  11   6  20  19]\n",
            " [ 14   6  22   0  10  18 301   1   4   2]\n",
            " [  5  14  20   8  11   1   2 321   1  28]\n",
            " [  5   8  21  18  13  22   5   4 275  13]\n",
            " [  5   2   1  12  23   3   4  16  11 314]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_112\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_112 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_224 (Dense)            (None, 62)                48670     \n",
            "_________________________________________________________________\n",
            "dense_225 (Dense)            (None, 10)                630       \n",
            "=================================================================\n",
            "Total params: 49,300\n",
            "Trainable params: 49,300\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 8.894842721929308e-06\n",
            "max validation accuracy 0.8130000233650208\n",
            "min validation loss 3.1371877193450928\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 8.894842721929308e-06\n",
            "last validation accuracy 0.812749981880188\n",
            "last validation loss 3.1548914909362793\n",
            "================================================================================\n",
            "Number of parameters = 80000, 100 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[336   0  12   0   5   5   6   1   1   4]\n",
            " [  0 433   3   4   1   1   4   0   4   0]\n",
            " [  7  15 344  14   4   4   5   9  13   3]\n",
            " [  6   3  17 305   4  41   1  13  11   7]\n",
            " [  1   1   5   2 356   1  10   5   7  30]\n",
            " [ 17   0   2  21  10 267  11   6  19  19]\n",
            " [ 15   5  22   0  10  18 300   2   4   2]\n",
            " [  6  14  20   8  11   1   2 322   0  27]\n",
            " [  5   7  22  18  13  22   5   4 275  13]\n",
            " [  5   2   1  12  23   3   5  16  11 313]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_113\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_113 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_226 (Dense)            (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_227 (Dense)            (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 8.054142199398484e-06\n",
            "max validation accuracy 0.812749981880188\n",
            "min validation loss 3.1509575843811035\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 8.054142199398484e-06\n",
            "last validation accuracy 0.812749981880188\n",
            "last validation loss 3.1730146408081055\n",
            "================================================================================\n",
            "Number of parameters = 150000, 188 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[335   0  12   0   5   6   6   1   1   4]\n",
            " [  0 433   3   4   1   1   4   0   4   0]\n",
            " [  7  15 344  14   4   4   5   9  13   3]\n",
            " [  6   3  15 308   4  41   1  13  10   7]\n",
            " [  1   2   5   2 358   1   9   5   6  29]\n",
            " [ 17   0   2  24  11 265  11   5  19  18]\n",
            " [ 14   6  22   0  10  19 299   2   4   2]\n",
            " [  5  14  20   9  11   1   2 322   0  27]\n",
            " [  5   7  22  18  13  23   5   4 274  13]\n",
            " [  5   2   1  12  24   3   4  17  11 312]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_114\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_114 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_228 (Dense)            (None, 188)               147580    \n",
            "_________________________________________________________________\n",
            "dense_229 (Dense)            (None, 10)                1890      \n",
            "=================================================================\n",
            "Total params: 149,470\n",
            "Trainable params: 149,470\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 7.1615672823099885e-06\n",
            "max validation accuracy 0.812749981880188\n",
            "min validation loss 3.167682647705078\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 7.1615672823099885e-06\n",
            "last validation accuracy 0.8125\n",
            "last validation loss 3.1785976886749268\n",
            "================================================================================\n",
            "Number of parameters = 300000, 377 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[334   0  12   0   5   5   8   1   1   4]\n",
            " [  0 433   4   4   1   1   4   0   3   0]\n",
            " [  7  14 345  14   4   4   5   9  13   3]\n",
            " [  5   2  15 309   4  42   1  12  11   7]\n",
            " [  1   2   6   2 357   1  10   3   6  30]\n",
            " [ 16   0   2  23  12 264  11   4  21  19]\n",
            " [ 12   5  24   0  12  19 298   2   4   2]\n",
            " [  5  15  21   7  11   1   1 325   0  25]\n",
            " [  5   7  22  21  12  24   5   4 271  13]\n",
            " [  5   1   1  11  24   4   4  18  11 312]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_115\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_115 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_230 (Dense)            (None, 377)               295945    \n",
            "_________________________________________________________________\n",
            "dense_231 (Dense)            (None, 10)                3780      \n",
            "=================================================================\n",
            "Total params: 299,725\n",
            "Trainable params: 299,725\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 6.574188773811329e-06\n",
            "max validation accuracy 0.812250018119812\n",
            "min validation loss 3.183006525039673\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 6.574188773811329e-06\n",
            "last validation accuracy 0.8119999766349792\n",
            "last validation loss 3.1966872215270996\n",
            "================================================================================\n",
            "Number of parameters = 800000, 1006 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[335   0  12   0   5   6   6   1   1   4]\n",
            " [  0 434   4   3   1   1   4   0   3   0]\n",
            " [  7  15 345  13   4   4   5   9  13   3]\n",
            " [  5   3  16 305   4  44   1  13  10   7]\n",
            " [  1   2   6   1 357   1  10   3   7  30]\n",
            " [ 14   0   2  24  10 267  12   6  19  18]\n",
            " [ 11   5  23   0  11  19 300   3   4   2]\n",
            " [  5  14  17   7  11   1   2 326   1  27]\n",
            " [  5   7  19  22  11  23   5   4 275  13]\n",
            " [  5   1   1  12  21   3   4  17  11 316]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_116\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_116 (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_232 (Dense)            (None, 1006)              789710    \n",
            "_________________________________________________________________\n",
            "dense_233 (Dense)            (None, 10)                10070     \n",
            "=================================================================\n",
            "Total params: 799,780\n",
            "Trainable params: 799,780\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 5.727602456317982e-06\n",
            "max validation accuracy 0.8149999976158142\n",
            "min validation loss 3.2226970195770264\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 5.727602456317982e-06\n",
            "last validation accuracy 0.8149999976158142\n",
            "last validation loss 3.222740888595581\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc73c48fb50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lBsxojT7dw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7ecdf2-b626-4aa1-dad0-2af60b94b176"
      },
      "source": [
        "# full MNIST\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# For networks smaller than the interpolation threshold, we decay the step size by 10% after each of 500 epochs\n",
        "# For these networks, training is stopped after classification error reached zero or 6000 epochs, whichever happens earlier. For networks larger than interpolation threshold, fixed step size is used, and training is stopped after 6000 epochs.\n",
        "def lr_decay(epoch, lr):\n",
        "    return lr * 0.9**(epoch//500)\n",
        "scheduler = tf.keras.callbacks.LearningRateScheduler(lr_decay)\n",
        "\n",
        "# For networks smaller than the interpolation threshold, training is stopped after classification error reached zero or 6000 epochs,\n",
        "class CustomCallback_epoch(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      if logs[\"accuracy\"]==1:\n",
        "        self.model.stop_training = True\n",
        "        print('STOPPING EARLY AFTER INTERPOLATING AT EPOCH %d' %epoch)\n",
        "        print('INTERPOLATION THRESHOLD REACHED')\n",
        "\n",
        "history_logs = []\n",
        "saved_weights = []\n",
        "\n",
        "num_parameters = [i*1000 for i in [3, 6, 9, 12, 24, 28, 32, 34, 36, 38, 40, 42, 46, 50, 80, 150, 300, 800]]\n",
        "hidden_sizes = [(N-10)//795 for N in num_parameters]\n",
        "for N in num_parameters:\n",
        "  print('='*80)\n",
        "  print('Number of parameters = %d, %d hidden layer neurons' %(N, (N-10)//795)) \n",
        "  print('='*80)\n",
        "\n",
        "  # P = (d+1)·H+(H+1)·K = 785H+10H+10 = 795H+10 --> H = (P-10)/795\n",
        "  num_nodes = (N-10)//795\n",
        "  # The remaining weights are initialized with normally distributed random numbers (mean 0 and variance 0.01). The smallest network is initialized using standard Glorotuniform distribution [19].\n",
        "  normal_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01**0.5)\n",
        "  glorot_initializer = tf.keras.initializers.GlorotUniform()  \n",
        "  # if this is the first model (smallest network) use standard Glorotuniform, otherwise use random normal\n",
        "  initializer = normal_initializer if saved_weights else glorot_initializer\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(num_nodes, activation='relu', kernel_initializer=initializer, bias_initializer=initializer),\n",
        "    tf.keras.layers.Dense(10, activation='softmax', kernel_initializer=initializer, bias_initializer=initializer)\n",
        "  ])\n",
        "\n",
        "  # paper used SGD with standard momentum (parameter value 0.95)\n",
        "  opt = tf.keras.optimizers.SGD(momentum=0.95)\n",
        "\n",
        "  model.compile(optimizer=opt,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  # print(model.weights[0].shape)\n",
        "  # print(model.weights[1].shape)\n",
        "  # # print(model.weights[1])\n",
        "  # print(model.weights[2].shape)\n",
        "  # # print(model.weights[2])\n",
        "  # print(model.weights[3].shape)\n",
        "  # # print(model.weights[3])\n",
        "\n",
        "  # from paper: To train a larger network with H2 > H1 hidden units, we initialize the first H1 hidden units of the larger network to the weights learned in the smaller network.  \n",
        "  # equivalently, here we expand saved weight with H1 units from prev model with randomly initialized weight for the new hidden units from the new model, then re-assign the combined weights to the new model\n",
        "  if saved_weights: \n",
        "    prev_weights = saved_weights[-1]\n",
        "    tf.compat.v1.assign(model.weights[0], tf.concat((prev_weights[0], model.weights[0][:, prev_weights[0].shape[1]:]), axis=1))\n",
        "    tf.compat.v1.assign(model.weights[1], tf.concat((prev_weights[1], model.weights[1][prev_weights[1].shape[0]:]), axis=0))\n",
        "    tf.compat.v1.assign(model.weights[2], tf.concat((prev_weights[2], model.weights[2][prev_weights[2].shape[0]:]), axis=0))\n",
        "    tf.compat.v1.assign(model.weights[3], prev_weights[3])\n",
        "\n",
        "# For networks smaller than the interpolation threshold, we decay the step size by 10% after each of 500 epochs,\n",
        "# For these networks, training is stopped after classification error reached zero or 6000 epochs, whichever happens earlier. For networks larger than interpolation threshold, fixed step size is used, and training is stopped after 6000 epochs.\n",
        "\n",
        "  interpolated = history_logs and max([h[1].history['accuracy'] for h in history_logs])[0] == 1.0\n",
        "  # interpolated = N >= 40000\n",
        "  if not interpolated:\n",
        "    history = model.fit(x_train, y_train, epochs=1000, validation_data=(x_test, y_test), verbose=0, callbacks=[CustomCallback_epoch(), scheduler])\n",
        "  else:\n",
        "    history = model.fit(x_train, y_train, epochs=1000, validation_data=(x_test, y_test), verbose=0)\n",
        "  \n",
        "  saved_weights.append(model.weights)\n",
        "\n",
        "  history_logs.append((N, history))\n",
        "  print(tf.math.confusion_matrix(y_test, tf.argmax(model.predict(x_test), axis=1)))\n",
        "  model.summary()\n",
        "  print('max training accuracy', max(history.history['accuracy']))\n",
        "  print('min training loss', min(history.history['loss']))\n",
        "  print('max validation accuracy', max(history.history['val_accuracy']))\n",
        "  print('min validation loss', min(history.history['val_loss']))\n",
        "  print()\n",
        "  print('last training accuracy', history.history['accuracy'][-1])\n",
        "  print('last training loss', history.history['loss'][-1])\n",
        "  print('last validation accuracy', history.history['val_accuracy'][-1])\n",
        "  print('last validation loss', history.history['val_loss'][-1])\n",
        "  \n",
        "# plot errors over number of parameters\n",
        "plt.title('Training and validation error')\n",
        "plt.plot([i[0]//1000 for i in history_logs], [1-i[1].history['val_accuracy'][-1] for i in history_logs], color='blue', label='val_error')\n",
        "plt.plot([i[0]//1000 for i in history_logs], [1-i[1].history['accuracy'][-1] for i in history_logs], color='orange', label='training error')\n",
        "plt.xlabel('Number of parameters N(*10^3)')\n",
        "plt.ylabel('Error')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "================================================================================\n",
            "Number of parameters = 3000, 3 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[ 899    0    1   12    2   57    6    1    2    0]\n",
            " [   0 1078   15    5    1    2    1    0   31    2]\n",
            " [   6   17  832   86    5   10   25    1   47    3]\n",
            " [   5    7   69  802    1   41    2   45   33    5]\n",
            " [   0    2    2    0  843    5   14    3   16   97]\n",
            " [  54    1   24   63   27  624   26   19   46    8]\n",
            " [  23    0   14    0    6   23  875    0   17    0]\n",
            " [   5   29    4   37    5    4    0  851    8   85]\n",
            " [   5   52   35   38   47   91   36   11  645   14]\n",
            " [   4    4    0    7  108   29    2   44    7  804]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 2355      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                40        \n",
            "=================================================================\n",
            "Total params: 2,395\n",
            "Trainable params: 2,395\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 0.8301500082015991\n",
            "min training loss 0.5735058188438416\n",
            "max validation accuracy 0.8260999917984009\n",
            "min validation loss 0.6023366451263428\n",
            "\n",
            "last training accuracy 0.830049991607666\n",
            "last training loss 0.57350754737854\n",
            "last validation accuracy 0.8252999782562256\n",
            "last validation loss 0.6025750637054443\n",
            "================================================================================\n",
            "Number of parameters = 6000, 7 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[ 949    0    2    2    1   10    7    4    4    1]\n",
            " [   0 1112    5    2    1    2    2    1   10    0]\n",
            " [  14    7  943   12    7    2   13   10   23    1]\n",
            " [   3    4   30  915    1   28    0   14    9    6]\n",
            " [   2    5   10    0  916    1   11    4    3   30]\n",
            " [  17    6    0   38    4  766   11    4   37    9]\n",
            " [  19    1    7    0    8    8  913    0    2    0]\n",
            " [   2   12   17    7    8    0    1  959    2   20]\n",
            " [   4   13   20    6    8   18   10   14  876    5]\n",
            " [   5    7    2   11   20   14    0   27    4  919]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7)                 5495      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                80        \n",
            "=================================================================\n",
            "Total params: 5,575\n",
            "Trainable params: 5,575\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 0.9435499906539917\n",
            "min training loss 0.19621846079826355\n",
            "max validation accuracy 0.9279999732971191\n",
            "min validation loss 0.2687876522541046\n",
            "\n",
            "last training accuracy 0.9434666633605957\n",
            "last training loss 0.19621878862380981\n",
            "last validation accuracy 0.926800012588501\n",
            "last validation loss 0.2709829807281494\n",
            "================================================================================\n",
            "Number of parameters = 9000, 11 hidden layer neurons\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YRfXkxxuKth"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}