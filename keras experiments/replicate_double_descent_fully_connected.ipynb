{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "replicate_double_descent_fully_connected.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "komPxXzy6oTT",
        "outputId": "511e7793-7c2d-4747-def1-a1755c62ca4d"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow.keras.layers import Dense, Flatten \n",
        "from tensorflow.keras import Model\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYkDDKhP6ozO",
        "outputId": "bd9e301a-a6e1-4a5a-aefd-d2ae7b2da9d1"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vqNW9NH6rvs"
      },
      "source": [
        "# paper's fully connected model learned on a subset of MNIST (n = 4000, d = 784, K = 10 classes)\n",
        "x_train, y_train = x_train[:4000], y_train[:4000]\n",
        "x_test, y_test = x_test[:4000], y_test[:4000]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6-hXm1t6vf_",
        "outputId": "73faf15b-81e9-4e31-eb21-03a95647536c"
      },
      "source": [
        "# For networks smaller than the interpolation threshold, we decay the step size by 10% after each of 500 epochs\n",
        "# For these networks, training is stopped after classification error reached zero or 6000 epochs, whichever happens earlier. For networks larger than interpolation threshold, fixed step size is used, and training is stopped after 6000 epochs.\n",
        "def lr_decay(epoch, lr):\n",
        "    return lr * 0.9**(epoch//500)\n",
        "scheduler = tf.keras.callbacks.LearningRateScheduler(lr_decay)\n",
        "\n",
        "# For networks smaller than the interpolation threshold, training is stopped after classification error reached zero or 6000 epochs,\n",
        "class CustomCallback_epoch(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      if logs[\"accuracy\"]==1:\n",
        "        self.model.stop_training = True\n",
        "        print('STOPPING EARLY AFTER INTERPOLATING AT EPOCH %d' %epoch)\n",
        "        print('INTERPOLATION THRESHOLD REACHED')\n",
        "\n",
        "history_logs = []\n",
        "saved_weights = []\n",
        "\n",
        "num_parameters = [i*1000 for i in [3, 6, 9, 12, 24, 28, 32, 34, 36, 38, 40, 42, 46, 50, 80, 150, 300, 800]]\n",
        "hidden_sizes = [(N-10)//795 for N in num_parameters]\n",
        "for N in num_parameters:\n",
        "  print('='*80)\n",
        "  print('Number of parameters = %d, %d hidden layer neurons' %(N, (N-10)//795)) \n",
        "  print('='*80)\n",
        "\n",
        "  # P = (d+1)·H+(H+1)·K = 785H+10H+10 = 795H+10 --> H = (P-10)/795\n",
        "  num_nodes = (N-10)//795\n",
        "  # The remaining weights are initialized with normally distributed random numbers (mean 0 and variance 0.01). The smallest network is initialized using standard Glorotuniform distribution [19].\n",
        "  normal_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01**0.5)\n",
        "  glorot_initializer = tf.keras.initializers.GlorotUniform()  \n",
        "  # if this is the first model (smallest network) use standard Glorotuniform, otherwise use random normal\n",
        "  initializer = normal_initializer if saved_weights else glorot_initializer\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(num_nodes, activation='relu', kernel_initializer=initializer, bias_initializer=initializer),\n",
        "    tf.keras.layers.Dense(10, activation='softmax', kernel_initializer=initializer, bias_initializer=initializer)\n",
        "  ])\n",
        "\n",
        "  # paper used SGD with standard momentum (parameter value 0.95)\n",
        "  opt = tf.keras.optimizers.SGD(momentum=0.95)\n",
        "\n",
        "  model.compile(optimizer=opt,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  # print(model.weights[0].shape)\n",
        "  # print(model.weights[1].shape)\n",
        "  # # print(model.weights[1])\n",
        "  # print(model.weights[2].shape)\n",
        "  # # print(model.weights[2])\n",
        "  # print(model.weights[3].shape)\n",
        "  # # print(model.weights[3])\n",
        "\n",
        "  # from paper: To train a larger network with H2 > H1 hidden units, we initialize the first H1 hidden units of the larger network to the weights learned in the smaller network.  \n",
        "  # equivalently, here we expand saved weight with H1 units from prev model with randomly initialized weight for the new hidden units from the new model, then re-assign the combined weights to the new model\n",
        "  if saved_weights: \n",
        "    prev_weights = saved_weights[-1]\n",
        "    tf.compat.v1.assign(model.weights[0], tf.concat((prev_weights[0], model.weights[0][:, prev_weights[0].shape[1]:]), axis=1))\n",
        "    tf.compat.v1.assign(model.weights[1], tf.concat((prev_weights[1], model.weights[1][prev_weights[1].shape[0]:]), axis=0))\n",
        "    tf.compat.v1.assign(model.weights[2], tf.concat((prev_weights[2], model.weights[2][prev_weights[2].shape[0]:]), axis=0))\n",
        "    tf.compat.v1.assign(model.weights[3], prev_weights[3])\n",
        "\n",
        "# For networks smaller than the interpolation threshold, we decay the step size by 10% after each of 500 epochs,\n",
        "# For these networks, training is stopped after classification error reached zero or 6000 epochs, whichever happens earlier. For networks larger than interpolation threshold, fixed step size is used, and training is stopped after 6000 epochs.\n",
        "\n",
        "  # interpolated = history_logs and max([h[1].history['loss'] for h in history_logs])[0] == 0.0\n",
        "  \n",
        "  # The expected interpolation threshold: paper observed it at n · K = 4000 * 10 = 40000\n",
        "  interpolated = N >= 40000\n",
        "\n",
        "  if not interpolated:\n",
        "    history = model.fit(x_train, y_train, epochs=6000, validation_data=(x_test, y_test), verbose=0, callbacks=[CustomCallback_epoch(), scheduler])\n",
        "  else:\n",
        "    history = model.fit(x_train, y_train, epochs=6000, validation_data=(x_test, y_test), verbose=0)\n",
        "  \n",
        "  saved_weights.append(model.weights)\n",
        "\n",
        "  history_logs.append((N, history))\n",
        "  print(tf.math.confusion_matrix(y_test, tf.argmax(model.predict(x_test), axis=1)))\n",
        "  model.summary()\n",
        "  print('max training accuracy', max(history.history['accuracy']))\n",
        "  print('min training loss', min(history.history['loss']))\n",
        "  print('max validation accuracy', max(history.history['val_accuracy']))\n",
        "  print('min validation loss', min(history.history['val_loss']))\n",
        "  print()\n",
        "  print('last training accuracy', history.history['accuracy'][-1])\n",
        "  print('last training loss', history.history['loss'][-1])\n",
        "  print('last validation accuracy', history.history['val_accuracy'][-1])\n",
        "  print('last validation loss', history.history['val_loss'][-1])\n",
        "  \n",
        "# plot errors over number of parameters\n",
        "plt.title('Training and validation error')\n",
        "plt.plot([i[0]//1000 for i in history_logs], [1-i[1].history['val_accuracy'][-1] for i in history_logs], color='blue', label='val_error')\n",
        "plt.plot([i[0]//1000 for i in history_logs], [1-i[1].history['accuracy'][-1] for i in history_logs], color='orange', label='training error')\n",
        "plt.xlabel('Number of parameters N(*10^3)')\n",
        "plt.ylabel('Error')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Number of parameters = 3000, 3 hidden layer neurons\n",
            "================================================================================\n",
            "tf.Tensor(\n",
            "[[264   0  33  14   1  28   7   0  23   0]\n",
            " [  0 413   1   7   5   7   3   5   5   4]\n",
            " [ 31   3 233  50   7  12   8  18  37  19]\n",
            " [ 11  26  13 238   3  48   0  36  28   5]\n",
            " [  0  18  13   0 310   0  13   1   7  56]\n",
            " [ 16  24   2  43   7 215  23   6  33   3]\n",
            " [ 12   3  28   1  22  27 263   1  16   5]\n",
            " [  0  22   9  13   6   3   0 301   6  51]\n",
            " [ 13  36  31  36  22  59  25  10 128  24]\n",
            " [  1  20  10   9  48   1   0  34  16 252]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_87\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_87 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_174 (Dense)            (None, 3)                 2355      \n",
            "_________________________________________________________________\n",
            "dense_175 (Dense)            (None, 10)                40        \n",
            "=================================================================\n",
            "Total params: 2,395\n",
            "Trainable params: 2,395\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 0.9079999923706055\n",
            "min training loss 0.3105255365371704\n",
            "max validation accuracy 0.7129999995231628\n",
            "min validation loss 0.9710409641265869\n",
            "\n",
            "last training accuracy 0.9075000286102295\n",
            "last training loss 0.31052568554878235\n",
            "last validation accuracy 0.6542500257492065\n",
            "last validation loss 2.0084891319274902\n",
            "================================================================================\n",
            "Number of parameters = 6000, 7 hidden layer neurons\n",
            "================================================================================\n",
            "STOPPING EARLY AFTER INTERPOLATING AT EPOCH 211\n",
            "INTERPOLATION THRESHOLD REACHED\n",
            "tf.Tensor(\n",
            "[[330   0  20   2   3   6   3   1   4   1]\n",
            " [  0 415   5   5   0   6   5   9   5   0]\n",
            " [  4  12 334  13   2   7   8  14  22   2]\n",
            " [  7   3  17 296   3  47   1  11  11  12]\n",
            " [  6   0  11   0 329   0   6  14   4  48]\n",
            " [ 12   2   5  30   5 259  13   4  33   9]\n",
            " [ 16   4  23   0  20  22 289   3   1   0]\n",
            " [  1  12  16  12  10   1   2 323   7  27]\n",
            " [  2   9  18  16  10  30   6   6 269  18]\n",
            " [  4   1   3  16  25   8   0  13  11 310]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_88\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_88 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_176 (Dense)            (None, 7)                 5495      \n",
            "_________________________________________________________________\n",
            "dense_177 (Dense)            (None, 10)                80        \n",
            "=================================================================\n",
            "Total params: 5,575\n",
            "Trainable params: 5,575\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 0.0022077022586017847\n",
            "max validation accuracy 0.7952499985694885\n",
            "min validation loss 1.4703683853149414\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 0.0022077022586017847\n",
            "last validation accuracy 0.7885000109672546\n",
            "last validation loss 2.458109140396118\n",
            "================================================================================\n",
            "Number of parameters = 9000, 11 hidden layer neurons\n",
            "================================================================================\n",
            "STOPPING EARLY AFTER INTERPOLATING AT EPOCH 5\n",
            "INTERPOLATION THRESHOLD REACHED\n",
            "tf.Tensor(\n",
            "[[329   0  21   2   2   6   4   1   4   1]\n",
            " [  0 415   5   5   0   6   6   9   4   0]\n",
            " [  4  12 336  13   2   7   8  14  20   2]\n",
            " [  6   3  18 299   4  45   2  10   9  12]\n",
            " [  5   0  11   0 332   0   6  14   4  46]\n",
            " [ 12   2   5  31   5 257  14   4  33   9]\n",
            " [ 15   4  23   0  19  20 293   3   1   0]\n",
            " [  1  12  16  12  10   1   2 323   7  27]\n",
            " [  1  10  21  16  10  30   6   6 266  18]\n",
            " [  4   1   3  14  26   8   0  13  11 311]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_89\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_89 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_178 (Dense)            (None, 11)                8635      \n",
            "_________________________________________________________________\n",
            "dense_179 (Dense)            (None, 10)                120       \n",
            "=================================================================\n",
            "Total params: 8,755\n",
            "Trainable params: 8,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 0.002171067986637354\n",
            "max validation accuracy 0.7902500033378601\n",
            "min validation loss 2.463792562484741\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 0.002171067986637354\n",
            "last validation accuracy 0.7902500033378601\n",
            "last validation loss 2.4720041751861572\n",
            "================================================================================\n",
            "Number of parameters = 12000, 15 hidden layer neurons\n",
            "================================================================================\n",
            "STOPPING EARLY AFTER INTERPOLATING AT EPOCH 1\n",
            "INTERPOLATION THRESHOLD REACHED\n",
            "tf.Tensor(\n",
            "[[330   0  20   2   2   6   4   1   4   1]\n",
            " [  0 417   5   5   0   6   6   7   4   0]\n",
            " [  4  12 336  13   2   7   8  14  20   2]\n",
            " [  6   3  18 297   4  46   3  10   9  12]\n",
            " [  6   0  11   0 332   0   6  13   4  46]\n",
            " [ 13   2   5  32   5 258  14   5  32   6]\n",
            " [ 16   4  23   0  20  20 291   3   1   0]\n",
            " [  1  12  16  12  10   1   2 325   7  25]\n",
            " [  2  10  20  16  10  30   7   8 265  16]\n",
            " [  4   1   3  16  27   8   0  14  11 307]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_90\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_90 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_180 (Dense)            (None, 15)                11775     \n",
            "_________________________________________________________________\n",
            "dense_181 (Dense)            (None, 10)                160       \n",
            "=================================================================\n",
            "Total params: 11,935\n",
            "Trainable params: 11,935\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 0.0021466263569891453\n",
            "max validation accuracy 0.7894999980926514\n",
            "min validation loss 2.4702799320220947\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 0.0021466263569891453\n",
            "last validation accuracy 0.7894999980926514\n",
            "last validation loss 2.470935821533203\n",
            "================================================================================\n",
            "Number of parameters = 24000, 30 hidden layer neurons\n",
            "================================================================================\n",
            "STOPPING EARLY AFTER INTERPOLATING AT EPOCH 0\n",
            "INTERPOLATION THRESHOLD REACHED\n",
            "tf.Tensor(\n",
            "[[330   0  21   2   2   5   4   1   4   1]\n",
            " [  0 416   4   6   0   6   6   8   4   0]\n",
            " [  4  12 336  13   2   7   8  14  20   2]\n",
            " [  6   3  18 301   3  44   3  11   7  12]\n",
            " [  6   0  11   0 330   0   6  14   4  47]\n",
            " [ 14   2   5  33   5 257  13   4  31   8]\n",
            " [ 16   4  23   0  20  21 290   3   1   0]\n",
            " [  1  12  16  12  10   1   2 325   6  26]\n",
            " [  2   9  18  18  10  30   6   7 265  19]\n",
            " [  4   1   4  18  25   7   0  14  10 308]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_91\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_91 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_182 (Dense)            (None, 30)                23550     \n",
            "_________________________________________________________________\n",
            "dense_183 (Dense)            (None, 10)                310       \n",
            "=================================================================\n",
            "Total params: 23,860\n",
            "Trainable params: 23,860\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 0.001988826086744666\n",
            "max validation accuracy 0.7894999980926514\n",
            "min validation loss 2.476426362991333\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 0.001988826086744666\n",
            "last validation accuracy 0.7894999980926514\n",
            "last validation loss 2.476426362991333\n",
            "================================================================================\n",
            "Number of parameters = 28000, 35 hidden layer neurons\n",
            "================================================================================\n",
            "STOPPING EARLY AFTER INTERPOLATING AT EPOCH 0\n",
            "INTERPOLATION THRESHOLD REACHED\n",
            "tf.Tensor(\n",
            "[[329   0  21   2   2   6   4   1   4   1]\n",
            " [  0 418   5   5   0   5   6   7   4   0]\n",
            " [  4  12 335  13   2   7   8  14  21   2]\n",
            " [  6   3  19 296   3  46   3  11   9  12]\n",
            " [  6   0  11   0 331   0   6  13   4  47]\n",
            " [ 14   2   5  32   5 258  13   4  32   7]\n",
            " [ 16   4  23   0  20  22 289   3   1   0]\n",
            " [  1  12  15  12  10   1   2 323   8  27]\n",
            " [  2   9  20  16  10  30   6   7 267  17]\n",
            " [  4   1   3  17  25   8   0  14  12 307]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_92\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_92 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_184 (Dense)            (None, 35)                27475     \n",
            "_________________________________________________________________\n",
            "dense_185 (Dense)            (None, 10)                360       \n",
            "=================================================================\n",
            "Total params: 27,835\n",
            "Trainable params: 27,835\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 0.0019257973181083798\n",
            "max validation accuracy 0.7882500290870667\n",
            "min validation loss 2.475721836090088\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 0.0019257973181083798\n",
            "last validation accuracy 0.7882500290870667\n",
            "last validation loss 2.475721836090088\n",
            "================================================================================\n",
            "Number of parameters = 32000, 40 hidden layer neurons\n",
            "================================================================================\n",
            "STOPPING EARLY AFTER INTERPOLATING AT EPOCH 0\n",
            "INTERPOLATION THRESHOLD REACHED\n",
            "tf.Tensor(\n",
            "[[329   0  21   2   2   6   4   1   4   1]\n",
            " [  0 417   5   5   0   5   6   8   4   0]\n",
            " [  4  12 335  13   2   7   8  14  21   2]\n",
            " [  6   3  19 296   3  46   3  11   9  12]\n",
            " [  6   0  11   0 331   0   6  13   4  47]\n",
            " [ 13   2   5  31   5 258  13   5  32   8]\n",
            " [ 16   4  23   0  21  22 289   2   1   0]\n",
            " [  1  12  16  12  10   1   2 325   7  25]\n",
            " [  2   9  19  17  10  31   6   6 266  18]\n",
            " [  4   1   3  17  25   8   0  14  12 307]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_93\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_93 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_186 (Dense)            (None, 40)                31400     \n",
            "_________________________________________________________________\n",
            "dense_187 (Dense)            (None, 10)                410       \n",
            "=================================================================\n",
            "Total params: 31,810\n",
            "Trainable params: 31,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 0.0018997011939063668\n",
            "max validation accuracy 0.7882500290870667\n",
            "min validation loss 2.4833436012268066\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 0.0018997011939063668\n",
            "last validation accuracy 0.7882500290870667\n",
            "last validation loss 2.4833436012268066\n",
            "================================================================================\n",
            "Number of parameters = 34000, 42 hidden layer neurons\n",
            "================================================================================\n",
            "STOPPING EARLY AFTER INTERPOLATING AT EPOCH 0\n",
            "INTERPOLATION THRESHOLD REACHED\n",
            "tf.Tensor(\n",
            "[[329   0  21   2   2   6   4   1   4   1]\n",
            " [  0 418   5   5   0   5   6   7   4   0]\n",
            " [  4  12 336  13   2   7   8  14  20   2]\n",
            " [  6   3  19 296   3  46   3  11   9  12]\n",
            " [  6   0  11   0 331   0   6  13   4  47]\n",
            " [ 13   2   5  32   5 258  13   5  32   7]\n",
            " [ 16   4  23   0  20  22 289   3   1   0]\n",
            " [  1  12  16  12  10   1   2 325   7  25]\n",
            " [  2   9  21  16  10  31   6   8 264  17]\n",
            " [  4   1   4  17  25   8   0  14  11 307]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_94\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_94 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_188 (Dense)            (None, 42)                32970     \n",
            "_________________________________________________________________\n",
            "dense_189 (Dense)            (None, 10)                430       \n",
            "=================================================================\n",
            "Total params: 33,400\n",
            "Trainable params: 33,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 0.0018761380342766643\n",
            "max validation accuracy 0.7882500290870667\n",
            "min validation loss 2.48813533782959\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 0.0018761380342766643\n",
            "last validation accuracy 0.7882500290870667\n",
            "last validation loss 2.48813533782959\n",
            "================================================================================\n",
            "Number of parameters = 36000, 45 hidden layer neurons\n",
            "================================================================================\n",
            "STOPPING EARLY AFTER INTERPOLATING AT EPOCH 0\n",
            "INTERPOLATION THRESHOLD REACHED\n",
            "tf.Tensor(\n",
            "[[329   0  21   2   2   6   4   1   4   1]\n",
            " [  0 418   5   5   0   5   6   7   4   0]\n",
            " [  4  12 336  13   2   7   8  14  20   2]\n",
            " [  6   3  19 296   3  46   3  11   9  12]\n",
            " [  6   0  11   0 332   0   6  12   4  47]\n",
            " [ 13   2   5  32   5 259  13   4  32   7]\n",
            " [ 16   4  23   0  21  22 289   2   1   0]\n",
            " [  1  12  16  12  10   1   2 324   7  26]\n",
            " [  2   9  21  16  10  31   6   7 264  18]\n",
            " [  4   1   4  17  26   8   0  13  11 307]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_95\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_95 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_190 (Dense)            (None, 45)                35325     \n",
            "_________________________________________________________________\n",
            "dense_191 (Dense)            (None, 10)                460       \n",
            "=================================================================\n",
            "Total params: 35,785\n",
            "Trainable params: 35,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 0.0018262813100591302\n",
            "max validation accuracy 0.7885000109672546\n",
            "min validation loss 2.4846608638763428\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 0.0018262813100591302\n",
            "last validation accuracy 0.7885000109672546\n",
            "last validation loss 2.4846608638763428\n",
            "================================================================================\n",
            "Number of parameters = 38000, 47 hidden layer neurons\n",
            "================================================================================\n",
            "STOPPING EARLY AFTER INTERPOLATING AT EPOCH 0\n",
            "INTERPOLATION THRESHOLD REACHED\n",
            "tf.Tensor(\n",
            "[[329   0  21   2   2   6   4   1   4   1]\n",
            " [  0 417   5   4   0   6   6   8   4   0]\n",
            " [  4  12 336  13   2   7   8  14  20   2]\n",
            " [  7   3  20 293   3  47   3  11   9  12]\n",
            " [  6   0  11   0 332   0   6  12   4  47]\n",
            " [ 13   2   5  31   5 260  13   5  32   6]\n",
            " [ 16   4  23   0  21  20 291   2   1   0]\n",
            " [  1  12  16  12  10   1   2 323   7  27]\n",
            " [  2   9  21  16  10  31   6   6 265  18]\n",
            " [  4   1   4  16  27   8   0  13  11 307]], shape=(10, 10), dtype=int32)\n",
            "Model: \"sequential_96\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_96 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_192 (Dense)            (None, 47)                36895     \n",
            "_________________________________________________________________\n",
            "dense_193 (Dense)            (None, 10)                480       \n",
            "=================================================================\n",
            "Total params: 37,375\n",
            "Trainable params: 37,375\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "max training accuracy 1.0\n",
            "min training loss 0.001795381773263216\n",
            "max validation accuracy 0.7882500290870667\n",
            "min validation loss 2.4857871532440186\n",
            "\n",
            "last training accuracy 1.0\n",
            "last training loss 0.001795381773263216\n",
            "last validation accuracy 0.7882500290870667\n",
            "last validation loss 2.4857871532440186\n",
            "================================================================================\n",
            "Number of parameters = 40000, 50 hidden layer neurons\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOaMYivifD5a"
      },
      "source": [
        "# full MNIST\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# For networks smaller than the interpolation threshold, we decay the step size by 10% after each of 500 epochs\n",
        "# For these networks, training is stopped after classification error reached zero or 6000 epochs, whichever happens earlier. For networks larger than interpolation threshold, fixed step size is used, and training is stopped after 6000 epochs.\n",
        "def lr_decay(epoch, lr):\n",
        "    return lr * 0.9**(epoch//500)\n",
        "scheduler = tf.keras.callbacks.LearningRateScheduler(lr_decay)\n",
        "\n",
        "# For networks smaller than the interpolation threshold, training is stopped after classification error reached zero or 6000 epochs,\n",
        "class CustomCallback_epoch(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      if logs[\"accuracy\"]==1:\n",
        "        self.model.stop_training = True\n",
        "        print('STOPPING EARLY AFTER INTERPOLATING AT EPOCH %d' %epoch)\n",
        "        print('INTERPOLATION THRESHOLD REACHED')\n",
        "\n",
        "history_logs = []\n",
        "saved_weights = []\n",
        "\n",
        "num_parameters = [i*1000 for i in [3, 6, 9, 12, 24, 28, 32, 34, 36, 38, 40, 42, 46, 50, 80, 150, 300, 800]]\n",
        "hidden_sizes = [(N-10)//795 for N in num_parameters]\n",
        "for N in num_parameters:\n",
        "  print('='*80)\n",
        "  print('Number of parameters = %d, %d hidden layer neurons' %(N, (N-10)//795)) \n",
        "  print('='*80)\n",
        "\n",
        "  # P = (d+1)·H+(H+1)·K = 785H+10H+10 = 795H+10 --> H = (P-10)/795\n",
        "  num_nodes = (N-10)//795\n",
        "  # The remaining weights are initialized with normally distributed random numbers (mean 0 and variance 0.01). The smallest network is initialized using standard Glorotuniform distribution [19].\n",
        "  normal_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01**0.5)\n",
        "  glorot_initializer = tf.keras.initializers.GlorotUniform()  \n",
        "  # if this is the first model (smallest network) use standard Glorotuniform, otherwise use random normal\n",
        "  initializer = normal_initializer if saved_weights else glorot_initializer\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(num_nodes, activation='relu', kernel_initializer=initializer, bias_initializer=initializer),\n",
        "    tf.keras.layers.Dense(10, activation='softmax', kernel_initializer=initializer, bias_initializer=initializer)\n",
        "  ])\n",
        "\n",
        "  # paper used SGD with standard momentum (parameter value 0.95)\n",
        "  opt = tf.keras.optimizers.SGD(momentum=0.95)\n",
        "\n",
        "  model.compile(optimizer=opt,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  # print(model.weights[0].shape)\n",
        "  # print(model.weights[1].shape)\n",
        "  # # print(model.weights[1])\n",
        "  # print(model.weights[2].shape)\n",
        "  # # print(model.weights[2])\n",
        "  # print(model.weights[3].shape)\n",
        "  # # print(model.weights[3])\n",
        "\n",
        "  # from paper: To train a larger network with H2 > H1 hidden units, we initialize the first H1 hidden units of the larger network to the weights learned in the smaller network.  \n",
        "  # equivalently, here we expand saved weight with H1 units from prev model with randomly initialized weight for the new hidden units from the new model, then re-assign the combined weights to the new model\n",
        "  if saved_weights: \n",
        "    prev_weights = saved_weights[-1]\n",
        "    tf.compat.v1.assign(model.weights[0], tf.concat((prev_weights[0], model.weights[0][:, prev_weights[0].shape[1]:]), axis=1))\n",
        "    tf.compat.v1.assign(model.weights[1], tf.concat((prev_weights[1], model.weights[1][prev_weights[1].shape[0]:]), axis=0))\n",
        "    tf.compat.v1.assign(model.weights[2], tf.concat((prev_weights[2], model.weights[2][prev_weights[2].shape[0]:]), axis=0))\n",
        "    tf.compat.v1.assign(model.weights[3], prev_weights[3])\n",
        "\n",
        "# For networks smaller than the interpolation threshold, we decay the step size by 10% after each of 500 epochs,\n",
        "# For these networks, training is stopped after classification error reached zero or 6000 epochs, whichever happens earlier. For networks larger than interpolation threshold, fixed step size is used, and training is stopped after 6000 epochs.\n",
        "\n",
        "  # interpolated = history_logs and max([h[1].history['loss'] for h in history_logs])[0] == 0.0\n",
        "  \n",
        "  # The expected interpolation threshold: paper observed it at n · K = 4000 * 10 = 40000\n",
        "  interpolated = N >= 40000\n",
        "\n",
        "  if not interpolated:\n",
        "    history = model.fit(x_train, y_train, epochs=6000, validation_data=(x_test, y_test), verbose=0, callbacks=[CustomCallback_epoch(), scheduler])\n",
        "  else:\n",
        "    history = model.fit(x_train, y_train, epochs=6000, validation_data=(x_test, y_test), verbose=0)\n",
        "  \n",
        "  saved_weights.append(model.weights)\n",
        "\n",
        "  history_logs.append((N, history))\n",
        "  print(tf.math.confusion_matrix(y_test, tf.argmax(model.predict(x_test), axis=1)))\n",
        "  model.summary()\n",
        "  print('max training accuracy', max(history.history['accuracy']))\n",
        "  print('min training loss', min(history.history['loss']))\n",
        "  print('max validation accuracy', max(history.history['val_accuracy']))\n",
        "  print('min validation loss', min(history.history['val_loss']))\n",
        "  print()\n",
        "  print('last training accuracy', history.history['accuracy'][-1])\n",
        "  print('last training loss', history.history['loss'][-1])\n",
        "  print('last validation accuracy', history.history['val_accuracy'][-1])\n",
        "  print('last validation loss', history.history['val_loss'][-1])\n",
        "  \n",
        "# plot errors over number of parameters\n",
        "plt.title('Training and validation error')\n",
        "plt.plot([i[0]//1000 for i in history_logs], [1-i[1].history['val_accuracy'][-1] for i in history_logs], color='blue', label='val_error')\n",
        "plt.plot([i[0]//1000 for i in history_logs], [1-i[1].history['accuracy'][-1] for i in history_logs], color='orange', label='training error')\n",
        "plt.xlabel('Number of parameters N(*10^3)')\n",
        "plt.ylabel('Error')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EVV7tFotq-d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}