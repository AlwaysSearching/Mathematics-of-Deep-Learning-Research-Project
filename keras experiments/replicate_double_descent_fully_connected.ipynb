{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "replicate_double_descent_fully_connected.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "komPxXzy6oTT",
        "outputId": "511e7793-7c2d-4747-def1-a1755c62ca4d"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow.keras.layers import Dense, Flatten \n",
        "from tensorflow.keras import Model\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYkDDKhP6ozO",
        "outputId": "bd9e301a-a6e1-4a5a-aefd-d2ae7b2da9d1"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vqNW9NH6rvs"
      },
      "source": [
        "# paper's fully connected model learned on a subset of MNIST (n = 4000, d = 784, K = 10 classes)\n",
        "x_train, y_train = x_train[:4000], y_train[:4000]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6-hXm1t6vf_",
        "outputId": "ab10f1d4-5f3b-4f0d-c239-5e2f954591e8"
      },
      "source": [
        "interpolated = False\n",
        "\n",
        "# For networks smaller than the interpolation threshold, we decay the step size by 10% after each of 500 epochs\n",
        "# For these networks, training is stopped after classification error reached zero or 6000 epochs, whichever happens earlier. For networks larger than interpolation threshold, fixed step size is used, and training is stopped after 6000 epochs.\n",
        "def lr_decay(epoch, lr):\n",
        "    return lr * 0.9**(epoch//500)\n",
        "scheduler = tf.keras.callbacks.LearningRateScheduler(lr_decay)\n",
        "\n",
        "# For networks smaller than the interpolation threshold, training is stopped after classification error reached zero or 6000 epochs,\n",
        "class CustomCallback_epoch(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      if logs[\"accuracy\"]==1:\n",
        "        self.model.stop_training = True\n",
        "        interpolated = True\n",
        "        print('STOPPING EARLY AFTER INTERPOLATING AT EPOCH %d' %epoch)\n",
        "        print('INTERPOLATION THRESHOLD REACHED')\n",
        "\n",
        "history_logs = []\n",
        "saved_weights = []\n",
        "\n",
        "num_parameters = [i*1000 for i in [1, 3, 6, 9, 12, 18, 24, 26, 28, 30, 32, 34, 35, 36, 37, 38, 39, 40, 42, 44, 46, 48, 50, 60, 80, 100, 150, 300, 800]]\n",
        "hidden_sizes = [(N-10)//795 for N in num_parameters]\n",
        "for N in num_parameters:\n",
        "  print('='*80)\n",
        "  print('Number of parameters = %d, %d hidden layer neurons' %(N, (N-10)//795)) \n",
        "  print('='*80)\n",
        "\n",
        "  # P = (d+1)·H+(H+1)·K = 785H+10H+10 = 795H+10 --> H = (P-10)/795\n",
        "  num_nodes = (N-10)//795\n",
        "  # The remaining weights are initialized with normally distributed random numbers (mean 0 and variance 0.01). The smallest network is initialized using standard Glorotuniform distribution [19].\n",
        "  normal_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01**0.5)\n",
        "  glorot_initializer = tf.keras.initializers.GlorotUniform()  \n",
        "  # if this is the first model (smallest network) use standard Glorotuniform, otherwise use random normal\n",
        "  initializer = normal_initializer if saved_weights else glorot_initializer\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(num_nodes, activation='relu', kernel_initializer=initializer, bias_initializer=initializer),\n",
        "    tf.keras.layers.Dense(10, activation='softmax', kernel_initializer=initializer, bias_initializer=initializer)\n",
        "  ])\n",
        "\n",
        "  # paper used SGD with standard momentum (parameter value 0.95)\n",
        "  opt = tf.keras.optimizers.SGD(momentum=0.95)\n",
        "\n",
        "  model.compile(optimizer=opt,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  # print(model.weights[0].shape)\n",
        "  # print(model.weights[1].shape)\n",
        "  # # print(model.weights[1])\n",
        "  # print(model.weights[2].shape)\n",
        "  # # print(model.weights[2])\n",
        "  # print(model.weights[3].shape)\n",
        "  # # print(model.weights[3])\n",
        "\n",
        "  # from paper: To train a larger network with H2 > H1 hidden units, we initialize the first H1 hidden units of the larger network to the weights learned in the smaller network.  \n",
        "  # equivalently, here we expand saved weight with H1 units from prev model with randomly initialized weight for the new hidden units from the new model, then re-assign the combined weights to the new model\n",
        "  if saved_weights: \n",
        "    prev_weights = saved_weights[-1]\n",
        "    tf.compat.v1.assign(model.weights[0], tf.concat((prev_weights[0], model.weights[0][:, prev_weights[0].shape[1]:]), axis=1))\n",
        "    tf.compat.v1.assign(model.weights[1], tf.concat((prev_weights[1], model.weights[1][prev_weights[1].shape[0]:]), axis=0))\n",
        "    tf.compat.v1.assign(model.weights[2], tf.concat((prev_weights[2], model.weights[2][prev_weights[2].shape[0]:]), axis=0))\n",
        "    tf.compat.v1.assign(model.weights[3], prev_weights[3])\n",
        "\n",
        "# For networks smaller than the interpolation threshold, we decay the step size by 10% after each of 500 epochs,\n",
        "# For these networks, training is stopped after classification error reached zero or 6000 epochs, whichever happens earlier. For networks larger than interpolation threshold, fixed step size is used, and training is stopped after 6000 epochs.\n",
        "\n",
        "  if not interpolated:\n",
        "    history = model.fit(x_train, y_train, epochs=6000, validation_data=(x_test, y_test), verbose=0, callbacks=[CustomCallback_epoch(), scheduler])\n",
        "  else:\n",
        "    history = model.fit(x_train, y_train, epochs=6000, validation_data=(x_test, y_test), verbose=0)\n",
        "  \n",
        "  saved_weights.append(model.weights)\n",
        "\n",
        "  history_logs.append((N, history))\n",
        "  print(tf.math.confusion_matrix(y_test, tf.argmax(model.predict(x_test), axis=1)))\n",
        "  model.summary()\n",
        "  print('max training accuracy', max(history.history['accuracy']))\n",
        "  print('min training loss', min(history.history['loss']))\n",
        "  print('max validation accuracy', max(history.history['val_accuracy']))\n",
        "  print('min validation loss', min(history.history['val_loss']))\n",
        "  print()\n",
        "  print('last training accuracy', history.history['accuracy'][-1])\n",
        "  print('last training loss', history.history['loss'][-1])\n",
        "  print('last validation accuracy', history.history['val_accuracy'][-1])\n",
        "  print('last validation loss', history.history['val_loss'][-1])\n",
        "  \n",
        "# plot errors over number of parameters\n",
        "plt.title('Training and validation error')\n",
        "plt.plot([i[0]//1000 for i in history_logs], [1-i[1].history['val_accuracy'][-1] for i in history_logs], color='blue', label='val_error')\n",
        "plt.plot([i[0]//1000 for i in history_logs], [1-i[1].history['accuracy'][-1] for i in history_logs], color='orange', label='training error')\n",
        "plt.xlabel('Number of parameters N(*10^3)')\n",
        "plt.ylabel('Error')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Number of parameters = 1000, 1 hidden layer neurons\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lBsxojT7dw0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}