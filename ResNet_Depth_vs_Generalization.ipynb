{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exterior-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from resnet import MNIST_ResNet\n",
    "from train_utils import Model_Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "logical-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST Data Set\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Normalize Image to be floats in range [0,1]\n",
    "def normalize_img(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "# Shuffle the Data set and set training batch size \n",
    "ds_train = ds_train.map(normalize_img)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples, reshuffle_each_iteration=True)\n",
    "ds_train = ds_train.batch(128)\n",
    "\n",
    "# Initilialize the Test Training data set\n",
    "ds_test = ds_test.map(normalize_img)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "referenced-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_ResNet(Model):\n",
    "    def __init__(self, input_shape, residual_block_params, n_classes=10, filter_n_0=64):\n",
    "        '''\n",
    "            Parameters\n",
    "            ----------\n",
    "                input_shape - int\n",
    "                    Input dimensions of image data\n",
    "                residual_block_params - dict\n",
    "                    List of dict specifying # filters, # residual blocks, and stride for each residual block in the network.  \n",
    "                    e.g. \n",
    "                        [{'n_filters': 16, 'block_depth':3, 'stride': 1}, {'n_filters': 32, 'block_depth':3, 'stride': 1}]\n",
    "                N_Classes - int\n",
    "                    Output dimension of the final softmax layer.\n",
    "                filter_n_0 - int\n",
    "                    Initial number of filters in the network prior to the Residual block layers.\n",
    "        '''\n",
    "        \n",
    "        super(MNIST_ResNet, self).__init__()     \n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.conv_1 = Conv2D(filters=filter_n_0, kernel_size=(5, 5), strides=2, padding=\"same\")\n",
    "        self.batch_norm_1 = BatchNormalization()\n",
    "        self.maxpool_1 = MaxPool2D(pool_size=(3, 3), strides=2, padding=\"same\")\n",
    "        \n",
    "        self.residual_blocks = []\n",
    "        \n",
    "        # Initialize the residual block layers using the parameter dictionaries.\n",
    "        for param_dict in residual_block_params:\n",
    "            self.residual_blocks.append(\n",
    "                make_residual_block_layer(**param_dict)\n",
    "            )\n",
    "\n",
    "        self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.softmax = tf.keras.layers.Dense(units=self.n_classes, activation=Softmax)\n",
    "                \n",
    "        \n",
    "    def call(self, x, training=None): \n",
    "        # Training is used for layers which utilize Batch Normalization.\n",
    "        \n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.batch_norm_1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        \n",
    "        # Pass through residual blocks\n",
    "        for residual_block in self.residual_blocks:\n",
    "            x = self.layer1(x, training=training)\n",
    "            \n",
    "        x = self.avgpool(x)\n",
    "        output = self.softmax(x)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, n_filters, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_1 = Conv2D(filters=n_filters, kernel_size=(3, 3), strides=stride, padding=\"same\")\n",
    "        self.batch_norm_1 = BatchNormalization()\n",
    "        \n",
    "        self.conv_2 = Conv2D(filters=n_filters, kernel_size=(3, 3), strides=1, padding=\"same\")\n",
    "        self.batch_norm_2 = BatchNormalization()\n",
    "        \n",
    "        # This is done for layers which reduce the filter dimensions. \n",
    "        # Use 1x1 convolutions when downsampling, and identity map otherwise.\n",
    "        if stride != 1:\n",
    "            self.downsample = tf.keras.Sequential()\n",
    "            self.downsample.add(Conv2D(filters=n_filters, kernel_size=(1, 1), strides=stride))\n",
    "            self.downsample.add(BatchNormalization())\n",
    "        else:\n",
    "            self.downsample = lambda x: x\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        residual = self.downsample(inputs)\n",
    "\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.batch_norm_1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        x = self.batch_norm_2(x, training=training)\n",
    "\n",
    "        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n",
    "        return output\n",
    "    \n",
    "def make_residual_block_layer(n_filters, block_depth, stride=1):\n",
    "    # Define a miniture network which is composed of sequential residual blocks with the same # of filters \n",
    "    res_block = tf.keras.Sequential()\n",
    "    res_block.add(ResidualBlock(n_filters, stride=stride))\n",
    "\n",
    "    for _ in range(block_depth):\n",
    "        res_block.add(ResidualBlock(n_filters, stride=1))\n",
    "\n",
    "    return res_block\n",
    "    \n",
    "    \n",
    "class Model_Train:\n",
    "    # Training Wrapper For Tensorflow Models. Allows a predifined model to be easily trained\n",
    "    # while also tracking parameter and gradient information.\n",
    "    \n",
    "    def __init__(self, Model):\n",
    "                \n",
    "        self.lr = 5e-4\n",
    "        self.n_classes = Model.n_classes       \n",
    "        \n",
    "        self.model = Model\n",
    "        self.init_loss()\n",
    "        self.init_optimizer()\n",
    "        \n",
    "        # Used to save the parameters of the model at a given point of time.\n",
    "        self.checkpoint = tf.train.Checkpoint(self.model)\n",
    "        self.checkpoint_path = self.model.__class__.__name__ + \"/training_checkpoints\"\n",
    "        \n",
    "        self.gradients\n",
    "        \n",
    "    \n",
    "    #initialize loss function and metrics to track over training\n",
    "    def init_loss(self):\n",
    "        self.loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "        self.train_confusion = tfa.metrics.MultiLabelConfusionMatrix(num_classes=self.n_classes, name='train_confusion_matrix')\n",
    "\n",
    "        self.test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "        self.test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "        self.test_confusion = tfa.metrics.MultiLabelConfusionMatrix(num_classes=self.n_classes, name='test_confusion_matrix')\n",
    "        \n",
    "\n",
    "    # Initialize Model optimizer\n",
    "    def init_optimizer(self):\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
    "    \n",
    "    # Take a single Training step on the given batch of training data.\n",
    "    @tf.function\n",
    "    def train_step(self, images, labels, track_gradient=False):\n",
    "        with tf.GradientTape() as gtape:\n",
    "            predictions = self.model(images, training=True)\n",
    "            loss = self.loss_function(labels, predictions)\n",
    "            \n",
    "        gradients = gtape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Track Gradient Information\n",
    "        \n",
    "        # Track model Performance\n",
    "        self.train_loss(loss)\n",
    "        self.train_accuracy(labels, predictions)\n",
    "        self.train_confusion(labels, predictions)\n",
    "        \n",
    "        return self.train_loss.result(), self.train_accuracy.result()*100, self.train_confusion.result()\n",
    "    \n",
    "    # Evaluate Model on Test Data\n",
    "    @tf.function\n",
    "    def test_step(self, images, labels):\n",
    "        predictions = self.model.predict(images)\n",
    "        test_loss = self.loss_function(labels, predictions)\n",
    "        \n",
    "        self.test_loss(test_loss)\n",
    "        self.test_accuracy(labels, predictions) \n",
    "        self.test_confusion(labels, predictions)\n",
    "        \n",
    "        return self.test_loss.result(), self.test_accuracy.result()*100, self.test_confusion.result()\n",
    "        \n",
    "    # Reset Metrics \n",
    "    @tf.function\n",
    "    def reset(self):\n",
    "        self.train_loss.reset_states()\n",
    "        self.train_accuracy.reset_states()\n",
    "        self.train_confusion.reset()\n",
    "        \n",
    "        self.test_loss.reset_states()\n",
    "        self.test_accuracy.reset_states()\n",
    "        self.test_confusion.reset()\n",
    "        \n",
    "    # Save a checkpoint instance of the model for later use\n",
    "    def model_checkpoint(self):\n",
    "        # Save a checkpoint to self.checkpoint_path-{save_counter}. Every time\n",
    "        # checkpoint.save is called, the save counter is increased.\n",
    "        save_path = checkpoint.save(self.checkpoint_path)\n",
    "        return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "assigned-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dim = ds_info.features['image'].shape\n",
    "n_classes = ds_info.features['label'].num_classes \n",
    "filter_n_0 = 16\n",
    "residual_block_params = [\n",
    "    {'n_filters': 16, 'block_depth':2, 'stride': 1},\n",
    "    {'n_filters': 32, 'block_depth':2, 'stride': 1}\n",
    "]\n",
    "\n",
    "base_model = MNIST_ResNet(\n",
    "    input_shape=image_dim,\n",
    "    residual_block_params=residual_block_params,\n",
    "    n_classes=n_classes,\n",
    "    filter_n_0=filter_n_0\n",
    ")\n",
    "trainer = Model_Trainer(base_model)\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    trainer.reset()\n",
    "    \n",
    "    for images, labels in ds_train:\n",
    "        train_loss, train_accuracy, train_confusion = trainer.train_step(images, labels)\n",
    "    \n",
    "    for images, labels in ds_test:\n",
    "        test_loss, test_accuracy, test_confusion = trainer.test_step(images, labels)\n",
    "    \n",
    "    template = 'Epoch {} - Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch, train_loss, train_accuracy, test_loss, test_accuracy))\n",
    "    \n",
    "    trainer.model_checkpoint()\n",
    "    trainer.log_metrics(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "strong-unemployment",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d44b2047d77b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mtrainable_variables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2302\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2303\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2304\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2306\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1924\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1925\u001b[0m     return self._dedup_weights(\n\u001b[1;32m-> 1926\u001b[1;33m         trackable_layer_utils.gather_trainable_weights(\n\u001b[0m\u001b[0;32m   1927\u001b[0m             \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1928\u001b[0m             \u001b[0msub_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\layer_utils.py\u001b[0m in \u001b[0;36mgather_trainable_weights\u001b[1;34m(trainable, sub_layers, extra_variables)\u001b[0m\n\u001b[0;32m    176\u001b[0m   \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msub_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m     \u001b[0mweights\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m   trainable_extra_variables = [\n\u001b[0;32m    180\u001b[0m       v for v in extra_variables if v.trainable]\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\data_structures.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m     return layer_utils.gather_trainable_weights(\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0msub_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\layer_utils.py\u001b[0m in \u001b[0;36mgather_trainable_weights\u001b[1;34m(trainable, sub_layers, extra_variables)\u001b[0m\n\u001b[0;32m    176\u001b[0m   \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msub_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m     \u001b[0mweights\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m   trainable_extra_variables = [\n\u001b[0;32m    180\u001b[0m       v for v in extra_variables if v.trainable]\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1922\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1924\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1925\u001b[0m     return self._dedup_weights(\n\u001b[0;32m   1926\u001b[0m         trackable_layer_utils.gather_trainable_weights(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36m_assert_weights_created\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[1;31m# When the graph has not been initialized, use the Model's implementation to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[1;31m# to check if the weights has been created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFunctional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=bad-super-call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_assert_weights_created\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2471\u001b[0m       \u001b[1;31m# been invoked yet, this will cover both sequential and subclass model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2472\u001b[0m       \u001b[1;31m# Also make sure to exclude Model class itself which has build() defined.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2473\u001b[1;33m       raise ValueError('Weights for model %s have not yet been created. '\n\u001b[0m\u001b[0;32m   2474\u001b[0m                        \u001b[1;34m'Weights are created when the Model is first called on '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2475\u001b[0m                        \u001b[1;34m'inputs or `build()` is called with an `input_shape`.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "base_model.trainable_variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
